{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_neural_network_modeling.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRql9YjD24Av"
      },
      "source": [
        "**Airbnb DC Hosting Helper**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmi-nMWw3D0R"
      },
      "source": [
        "**6_neural_network_modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWPGSqtQ3D-I"
      },
      "source": [
        "**Summary**\n",
        "\n",
        "This notebook will explore using Neural Network modeling on this binary classification problem to see how it compares with other types of classification models. This notebook was created in Google Colab which is why it is a separate notebook in the modeling phase. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdUBtNJsy5r2"
      },
      "source": [
        "Import libraries and read in data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHUbVxhmuCPD"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5FnInAKuQIB",
        "outputId": "09b866c3-3e71-470e-c6ab-a0e5791ade14"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZuCxcz9vkez"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/final_df.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJMYxZNMvka8",
        "outputId": "1e417232-e119-421f-ea8e-8003da683512"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3652 entries, 0 to 3651\n",
            "Data columns (total 100 columns):\n",
            " #   Column                                    Non-Null Count  Dtype  \n",
            "---  ------                                    --------------  -----  \n",
            " 0   Unnamed: 0                                3652 non-null   int64  \n",
            " 1   id                                        3652 non-null   int64  \n",
            " 2   name                                      3652 non-null   object \n",
            " 3   description                               3609 non-null   object \n",
            " 4   neighborhood_overview                     2794 non-null   object \n",
            " 5   host_id                                   3652 non-null   int64  \n",
            " 6   host_about                                2529 non-null   object \n",
            " 7   host_response_time                        3652 non-null   object \n",
            " 8   host_response_rate                        3652 non-null   float64\n",
            " 9   host_acceptance_rate                      3652 non-null   float64\n",
            " 10  host_is_superhost                         3652 non-null   int64  \n",
            " 11  host_has_profile_pic                      3652 non-null   int64  \n",
            " 12  host_identity_verified                    3652 non-null   int64  \n",
            " 13  neighbourhood_cleansed                    3652 non-null   object \n",
            " 14  latitude_x                                3652 non-null   float64\n",
            " 15  longitude_x                               3652 non-null   float64\n",
            " 16  room_type                                 3652 non-null   object \n",
            " 17  accommodates                              3652 non-null   int64  \n",
            " 18  bathrooms                                 3652 non-null   float64\n",
            " 19  bedrooms                                  3652 non-null   float64\n",
            " 20  beds                                      3652 non-null   float64\n",
            " 21  amenities                                 3652 non-null   object \n",
            " 22  price                                     3652 non-null   float64\n",
            " 23  minimum_nights                            3652 non-null   int64  \n",
            " 24  maximum_nights                            3652 non-null   int64  \n",
            " 25  minimum_nights_avg_ntm                    3652 non-null   float64\n",
            " 26  maximum_nights_avg_ntm                    3652 non-null   float64\n",
            " 27  availability_30                           3652 non-null   int64  \n",
            " 28  availability_60                           3652 non-null   int64  \n",
            " 29  availability_90                           3652 non-null   int64  \n",
            " 30  availability_365                          3652 non-null   int64  \n",
            " 31  instant_bookable                          3652 non-null   int64  \n",
            " 32  calculated_host_listings_count            3652 non-null   int64  \n",
            " 33  historic site                             3652 non-null   int64  \n",
            " 34  museum                                    3652 non-null   int64  \n",
            " 35  metro                                     3652 non-null   int64  \n",
            " 36  music venue                               3652 non-null   int64  \n",
            " 37  perfomring arts venue                     3652 non-null   int64  \n",
            " 38  college and university                    3652 non-null   int64  \n",
            " 39  food                                      3652 non-null   int64  \n",
            " 40  nightlife spot                            3652 non-null   int64  \n",
            " 41  outdoors and recreation                   3652 non-null   int64  \n",
            " 42  government building                       3652 non-null   int64  \n",
            " 43  clothing store                            3652 non-null   int64  \n",
            " 44  popular                                   3652 non-null   int64  \n",
            " 45  days_being_host                           3652 non-null   int64  \n",
            " 46  days_since_first_review                   3652 non-null   int64  \n",
            " 47  days_since_last_review                    3652 non-null   int64  \n",
            " 48  name_word_count                           3652 non-null   int64  \n",
            " 49  name_neutral_sentiment                    3652 non-null   float64\n",
            " 50  name_negative_sentiment                   3652 non-null   float64\n",
            " 51  name_positive_sentiment                   3652 non-null   float64\n",
            " 52  name_compound_sentiment                   3652 non-null   float64\n",
            " 53  description_word_count                    3652 non-null   int64  \n",
            " 54  description_neutral_sentiment             3652 non-null   float64\n",
            " 55  description_negative_sentiment            3652 non-null   float64\n",
            " 56  description_positive_sentiment            3652 non-null   float64\n",
            " 57  description_compound_sentiment            3652 non-null   float64\n",
            " 58  neighborhood_overview_word_count          3652 non-null   int64  \n",
            " 59  neighborhood_overview_neutral_sentiment   3652 non-null   float64\n",
            " 60  neighborhood_overview_negative_sentiment  3652 non-null   float64\n",
            " 61  neighborhood_overview_positive_sentiment  3652 non-null   float64\n",
            " 62  neighborhood_overview_compound_sentiment  3652 non-null   float64\n",
            " 63  host_about_word_count                     3652 non-null   int64  \n",
            " 64  host_about_neutral_sentiment              3652 non-null   float64\n",
            " 65  host_about_negative_sentiment             3652 non-null   float64\n",
            " 66  host_about_positive_sentiment             3652 non-null   float64\n",
            " 67  host_about_compound_sentiment             3652 non-null   float64\n",
            " 68  wifi                                      3652 non-null   int64  \n",
            " 69  smoke alarm                               3652 non-null   int64  \n",
            " 70  essentials                                3652 non-null   int64  \n",
            " 71  heating                                   3652 non-null   int64  \n",
            " 72  air conditioning                          3652 non-null   int64  \n",
            " 73  hangers                                   3652 non-null   int64  \n",
            " 74  iron                                      3652 non-null   int64  \n",
            " 75  kitchen                                   3652 non-null   int64  \n",
            " 76  long term stays allowed                   3652 non-null   int64  \n",
            " 77  hair dryer                                3652 non-null   int64  \n",
            " 78  carbon monoxide alarm                     3652 non-null   int64  \n",
            " 79  hot water                                 3652 non-null   int64  \n",
            " 80  shampoo                                   3652 non-null   int64  \n",
            " 81  dedicated workspace                       3652 non-null   int64  \n",
            " 82  dishes and silverware                     3652 non-null   int64  \n",
            " 83  microwave                                 3652 non-null   int64  \n",
            " 84  washer                                    3652 non-null   int64  \n",
            " 85  dryer                                     3652 non-null   int64  \n",
            " 86  fire extinguisher                         3652 non-null   int64  \n",
            " 87  refrigerator                              3652 non-null   int64  \n",
            " 88  coffee maker                              3652 non-null   int64  \n",
            " 89  cooking basics                            3652 non-null   int64  \n",
            " 90  private entrance                          3652 non-null   int64  \n",
            " 91  bed linens                                3652 non-null   int64  \n",
            " 92  stove                                     3652 non-null   int64  \n",
            " 93  oven                                      3652 non-null   int64  \n",
            " 94  free street parking                       3652 non-null   int64  \n",
            " 95  dishwasher                                3652 non-null   int64  \n",
            " 96  first aid kit                             3652 non-null   int64  \n",
            " 97  extra pillows and blankets                3652 non-null   int64  \n",
            " 98  tv                                        3652 non-null   int64  \n",
            " 99  patio or balcony                          3652 non-null   int64  \n",
            "dtypes: float64(26), int64(66), object(8)\n",
            "memory usage: 2.8+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi7jPzbGy-Ng"
      },
      "source": [
        "Clean data before modeling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6wY4pMCvkVB"
      },
      "source": [
        "df.drop(columns = ['Unnamed: 0', 'id', 'host_id',  'amenities', 'latitude_x', 'longitude_x',\n",
        "                   'name', 'description', 'neighborhood_overview', 'host_about'\n",
        "                  ], inplace=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpXEfs7JvkQb"
      },
      "source": [
        "cols = df.columns\n",
        "\n",
        "new_cols = []\n",
        "\n",
        "for i in cols:\n",
        "    new_cols.append(i.replace(' ','_'))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLiGcbEJvkLC"
      },
      "source": [
        "df.columns = new_cols"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tpybx7I1wDK2"
      },
      "source": [
        "normal = df.drop(columns=  ['host_response_time', 'neighbourhood_cleansed', 'room_type'])\n",
        "\n",
        "ohe_cats = ['host_response_time', 'neighbourhood_cleansed', 'room_type']\n",
        "\n",
        "categorical = df[ohe_cats]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72rdCP5WwDND"
      },
      "source": [
        "enc = OneHotEncoder(drop = 'first', sparse=False)\n",
        "\n",
        "categorical = enc.fit_transform(categorical) "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LDCi65EwDPc"
      },
      "source": [
        "dummy_cat_names = enc.get_feature_names(ohe_cats)\n",
        "\n",
        "categorical_df = pd.DataFrame(categorical, columns=dummy_cat_names)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC9M_WcCwDRd"
      },
      "source": [
        "df = pd.concat([normal, categorical_df], axis=1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "jl-n2iwwwDUg",
        "outputId": "8d0429ea-10c9-417a-a167-1f5f67f071ee"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>host_response_rate</th>\n",
              "      <th>host_acceptance_rate</th>\n",
              "      <th>host_is_superhost</th>\n",
              "      <th>host_has_profile_pic</th>\n",
              "      <th>host_identity_verified</th>\n",
              "      <th>accommodates</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>beds</th>\n",
              "      <th>price</th>\n",
              "      <th>minimum_nights</th>\n",
              "      <th>maximum_nights</th>\n",
              "      <th>minimum_nights_avg_ntm</th>\n",
              "      <th>maximum_nights_avg_ntm</th>\n",
              "      <th>availability_30</th>\n",
              "      <th>availability_60</th>\n",
              "      <th>availability_90</th>\n",
              "      <th>availability_365</th>\n",
              "      <th>instant_bookable</th>\n",
              "      <th>calculated_host_listings_count</th>\n",
              "      <th>historic_site</th>\n",
              "      <th>museum</th>\n",
              "      <th>metro</th>\n",
              "      <th>music_venue</th>\n",
              "      <th>perfomring_arts_venue</th>\n",
              "      <th>college_and_university</th>\n",
              "      <th>food</th>\n",
              "      <th>nightlife_spot</th>\n",
              "      <th>outdoors_and_recreation</th>\n",
              "      <th>government_building</th>\n",
              "      <th>clothing_store</th>\n",
              "      <th>popular</th>\n",
              "      <th>days_being_host</th>\n",
              "      <th>days_since_first_review</th>\n",
              "      <th>days_since_last_review</th>\n",
              "      <th>name_word_count</th>\n",
              "      <th>name_neutral_sentiment</th>\n",
              "      <th>name_negative_sentiment</th>\n",
              "      <th>name_positive_sentiment</th>\n",
              "      <th>name_compound_sentiment</th>\n",
              "      <th>...</th>\n",
              "      <th>neighbourhood_cleansed_Capitol Hill, Lincoln Park</th>\n",
              "      <th>neighbourhood_cleansed_Capitol View, Marshall Heights, Benning Heights</th>\n",
              "      <th>neighbourhood_cleansed_Cathedral Heights, McLean Gardens, Glover Park</th>\n",
              "      <th>neighbourhood_cleansed_Cleveland Park, Woodley Park, Massachusetts Avenue Heights, Woodland-Normanstone Terrace</th>\n",
              "      <th>neighbourhood_cleansed_Colonial Village, Shepherd Park, North Portal Estates</th>\n",
              "      <th>neighbourhood_cleansed_Columbia Heights, Mt. Pleasant, Pleasant Plains, Park View</th>\n",
              "      <th>neighbourhood_cleansed_Congress Heights, Bellevue, Washington Highlands</th>\n",
              "      <th>neighbourhood_cleansed_Deanwood, Burrville, Grant Park, Lincoln Heights, Fairmont Heights</th>\n",
              "      <th>neighbourhood_cleansed_Douglas, Shipley Terrace</th>\n",
              "      <th>neighbourhood_cleansed_Downtown, Chinatown, Penn Quarters, Mount Vernon Square, North Capitol Street</th>\n",
              "      <th>neighbourhood_cleansed_Dupont Circle, Connecticut Avenue/K Street</th>\n",
              "      <th>neighbourhood_cleansed_Eastland Gardens, Kenilworth</th>\n",
              "      <th>neighbourhood_cleansed_Edgewood, Bloomingdale, Truxton Circle, Eckington</th>\n",
              "      <th>neighbourhood_cleansed_Fairfax Village, Naylor Gardens, Hillcrest, Summit Park</th>\n",
              "      <th>neighbourhood_cleansed_Friendship Heights, American University Park, Tenleytown</th>\n",
              "      <th>neighbourhood_cleansed_Georgetown, Burleith/Hillandale</th>\n",
              "      <th>neighbourhood_cleansed_Hawthorne, Barnaby Woods, Chevy Chase</th>\n",
              "      <th>neighbourhood_cleansed_Historic Anacostia</th>\n",
              "      <th>neighbourhood_cleansed_Howard University, Le Droit Park, Cardozo/Shaw</th>\n",
              "      <th>neighbourhood_cleansed_Ivy City, Arboretum, Trinidad, Carver Langston</th>\n",
              "      <th>neighbourhood_cleansed_Kalorama Heights, Adams Morgan, Lanier Heights</th>\n",
              "      <th>neighbourhood_cleansed_Lamont Riggs, Queens Chapel, Fort Totten, Pleasant Hill</th>\n",
              "      <th>neighbourhood_cleansed_Mayfair, Hillbrook, Mahaning Heights</th>\n",
              "      <th>neighbourhood_cleansed_Near Southeast, Navy Yard</th>\n",
              "      <th>neighbourhood_cleansed_North Cleveland Park, Forest Hills, Van Ness</th>\n",
              "      <th>neighbourhood_cleansed_North Michigan Park, Michigan Park, University Heights</th>\n",
              "      <th>neighbourhood_cleansed_River Terrace, Benning, Greenway, Dupont Park</th>\n",
              "      <th>neighbourhood_cleansed_Shaw, Logan Circle</th>\n",
              "      <th>neighbourhood_cleansed_Sheridan, Barry Farm, Buena Vista</th>\n",
              "      <th>neighbourhood_cleansed_Southwest Employment Area, Southwest/Waterfront, Fort McNair, Buzzard Point</th>\n",
              "      <th>neighbourhood_cleansed_Spring Valley, Palisades, Wesley Heights, Foxhall Crescent, Foxhall Village, Georgetown Reservoir</th>\n",
              "      <th>neighbourhood_cleansed_Takoma, Brightwood, Manor Park</th>\n",
              "      <th>neighbourhood_cleansed_Twining, Fairlawn, Randle Highlands, Penn Branch, Fort Davis Park, Fort Dupont</th>\n",
              "      <th>neighbourhood_cleansed_Union Station, Stanton Park, Kingman Park</th>\n",
              "      <th>neighbourhood_cleansed_West End, Foggy Bottom, GWU</th>\n",
              "      <th>neighbourhood_cleansed_Woodland/Fort Stanton, Garfield Heights, Knox Hill</th>\n",
              "      <th>neighbourhood_cleansed_Woodridge, Fort Lincoln, Gateway</th>\n",
              "      <th>room_type_Hotel room</th>\n",
              "      <th>room_type_Private room</th>\n",
              "      <th>room_type_Shared room</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.8</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>2</td>\n",
              "      <td>365</td>\n",
              "      <td>2.0</td>\n",
              "      <td>365.0</td>\n",
              "      <td>1</td>\n",
              "      <td>31</td>\n",
              "      <td>61</td>\n",
              "      <td>336</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>25</td>\n",
              "      <td>5</td>\n",
              "      <td>28</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>4610</td>\n",
              "      <td>2576</td>\n",
              "      <td>180</td>\n",
              "      <td>3</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>7</td>\n",
              "      <td>200</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1125.0</td>\n",
              "      <td>9</td>\n",
              "      <td>20</td>\n",
              "      <td>50</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>40</td>\n",
              "      <td>18</td>\n",
              "      <td>4</td>\n",
              "      <td>29</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4437</td>\n",
              "      <td>1912</td>\n",
              "      <td>16</td>\n",
              "      <td>6</td>\n",
              "      <td>0.325</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.675</td>\n",
              "      <td>0.8519</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>185.0</td>\n",
              "      <td>2</td>\n",
              "      <td>180</td>\n",
              "      <td>2.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>17</td>\n",
              "      <td>47</td>\n",
              "      <td>76</td>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>36</td>\n",
              "      <td>28</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>46</td>\n",
              "      <td>44</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>45</td>\n",
              "      <td>48</td>\n",
              "      <td>47</td>\n",
              "      <td>0</td>\n",
              "      <td>4346</td>\n",
              "      <td>2223</td>\n",
              "      <td>19</td>\n",
              "      <td>4</td>\n",
              "      <td>0.455</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.545</td>\n",
              "      <td>0.5574</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>1</td>\n",
              "      <td>365</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1125.0</td>\n",
              "      <td>12</td>\n",
              "      <td>42</td>\n",
              "      <td>72</td>\n",
              "      <td>347</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>17</td>\n",
              "      <td>48</td>\n",
              "      <td>27</td>\n",
              "      <td>44</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4347</td>\n",
              "      <td>3929</td>\n",
              "      <td>23</td>\n",
              "      <td>3</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>1</td>\n",
              "      <td>365</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1125.0</td>\n",
              "      <td>19</td>\n",
              "      <td>49</td>\n",
              "      <td>79</td>\n",
              "      <td>354</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>17</td>\n",
              "      <td>48</td>\n",
              "      <td>27</td>\n",
              "      <td>44</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4347</td>\n",
              "      <td>2121</td>\n",
              "      <td>28</td>\n",
              "      <td>4</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 132 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   host_response_rate  ...  room_type_Shared room\n",
              "0                 0.8  ...                    0.0\n",
              "1                 1.0  ...                    0.0\n",
              "2                 1.0  ...                    0.0\n",
              "3                 1.0  ...                    0.0\n",
              "4                 1.0  ...                    0.0\n",
              "\n",
              "[5 rows x 132 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "px6UDk2pwQWT",
        "outputId": "dd9ee362-2470-48a9-dcd4-ae031ca690db"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3652, 132)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zunA1XWPzCA7"
      },
      "source": [
        "Baseline score to know for modeling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzcsdyDMwUMS",
        "outputId": "a4a0dfe2-d609-4c69-cab2-8af542ce8ec4"
      },
      "source": [
        "df['popular'].value_counts(normalize=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.776835\n",
              "1    0.223165\n",
              "Name: popular, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DLcDYlKzETz"
      },
      "source": [
        "Split data into training and testing groups and scale data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMYPB8MKw4Eh"
      },
      "source": [
        "X = df.drop(columns=['popular'])\n",
        "y = df['popular']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y )"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fc-LTTWw4UW"
      },
      "source": [
        "sc = StandardScaler()\n",
        "\n",
        "X_train_sc = sc.fit_transform(X_train)\n",
        "\n",
        "X_test_sc = sc.transform(X_test)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmVaoVoPzH_w"
      },
      "source": [
        "Set random seed, create and fit model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SHh0CKlw0kT"
      },
      "source": [
        "tf.random.set_seed(13)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GQ92e9uxGJq",
        "outputId": "d009446f-69c8-44a5-ba4b-2185bce90483"
      },
      "source": [
        "n_input = X_train_sc.shape[1]\n",
        "\n",
        "n_input"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mheae4C5xGLo"
      },
      "source": [
        "#network type, feet foward, fully squential network for binary classification\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "#layer1\n",
        "model.add(Dense(n_input, input_shape= (n_input,) , activation='relu'))\n",
        "\n",
        "#layer 2\n",
        "model.add(Dense(n_input, activation='relu'))\n",
        "\n",
        "#layer 3\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-HjLNeZxGOO"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3E5LKtxyxGQj",
        "outputId": "c00d2085-be09-4170-d2a1-36e829b50244"
      },
      "source": [
        "h = model.fit(X_train_sc, y_train, validation_data = (X_test_sc, y_test), epochs = 100, batch_size=512);"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "6/6 [==============================] - 1s 36ms/step - loss: 0.5905 - accuracy: 0.6871 - val_loss: 26.5437 - val_accuracy: 0.7766\n",
            "Epoch 2/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.4921 - accuracy: 0.7769 - val_loss: 29.3149 - val_accuracy: 0.7777\n",
            "Epoch 3/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.4263 - accuracy: 0.7977 - val_loss: 23.3438 - val_accuracy: 0.7908\n",
            "Epoch 4/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.4021 - accuracy: 0.8258 - val_loss: 28.3594 - val_accuracy: 0.7974\n",
            "Epoch 5/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.3688 - accuracy: 0.8339 - val_loss: 39.3398 - val_accuracy: 0.8094\n",
            "Epoch 6/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.3474 - accuracy: 0.8386 - val_loss: 42.3731 - val_accuracy: 0.8182\n",
            "Epoch 7/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.3262 - accuracy: 0.8558 - val_loss: 44.1546 - val_accuracy: 0.8226\n",
            "Epoch 8/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.3084 - accuracy: 0.8649 - val_loss: 50.1546 - val_accuracy: 0.8302\n",
            "Epoch 9/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.2926 - accuracy: 0.8708 - val_loss: 54.6395 - val_accuracy: 0.8324\n",
            "Epoch 10/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2798 - accuracy: 0.8784 - val_loss: 57.0576 - val_accuracy: 0.8324\n",
            "Epoch 11/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2669 - accuracy: 0.8865 - val_loss: 60.1592 - val_accuracy: 0.8379\n",
            "Epoch 12/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2559 - accuracy: 0.8963 - val_loss: 61.1125 - val_accuracy: 0.8313\n",
            "Epoch 13/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2452 - accuracy: 0.9007 - val_loss: 66.5064 - val_accuracy: 0.8357\n",
            "Epoch 14/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2348 - accuracy: 0.9120 - val_loss: 66.4269 - val_accuracy: 0.8302\n",
            "Epoch 15/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.2248 - accuracy: 0.9153 - val_loss: 73.0013 - val_accuracy: 0.8324\n",
            "Epoch 16/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2159 - accuracy: 0.9186 - val_loss: 73.9577 - val_accuracy: 0.8313\n",
            "Epoch 17/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.2071 - accuracy: 0.9237 - val_loss: 75.8796 - val_accuracy: 0.8335\n",
            "Epoch 18/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1984 - accuracy: 0.9288 - val_loss: 79.5841 - val_accuracy: 0.8302\n",
            "Epoch 19/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1896 - accuracy: 0.9295 - val_loss: 82.7457 - val_accuracy: 0.8302\n",
            "Epoch 20/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1815 - accuracy: 0.9357 - val_loss: 84.2349 - val_accuracy: 0.8324\n",
            "Epoch 21/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1718 - accuracy: 0.9401 - val_loss: 85.8201 - val_accuracy: 0.8324\n",
            "Epoch 22/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1648 - accuracy: 0.9478 - val_loss: 86.7764 - val_accuracy: 0.8324\n",
            "Epoch 23/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1567 - accuracy: 0.9449 - val_loss: 92.0028 - val_accuracy: 0.8291\n",
            "Epoch 24/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1488 - accuracy: 0.9500 - val_loss: 95.8587 - val_accuracy: 0.8302\n",
            "Epoch 25/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1397 - accuracy: 0.9573 - val_loss: 103.8116 - val_accuracy: 0.8313\n",
            "Epoch 26/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1329 - accuracy: 0.9609 - val_loss: 108.2146 - val_accuracy: 0.8280\n",
            "Epoch 27/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1251 - accuracy: 0.9657 - val_loss: 112.7551 - val_accuracy: 0.8226\n",
            "Epoch 28/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1177 - accuracy: 0.9679 - val_loss: 116.3820 - val_accuracy: 0.8215\n",
            "Epoch 29/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1115 - accuracy: 0.9682 - val_loss: 121.8298 - val_accuracy: 0.8248\n",
            "Epoch 30/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1037 - accuracy: 0.9730 - val_loss: 123.4981 - val_accuracy: 0.8248\n",
            "Epoch 31/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0979 - accuracy: 0.9785 - val_loss: 127.6313 - val_accuracy: 0.8258\n",
            "Epoch 32/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0911 - accuracy: 0.9799 - val_loss: 137.1960 - val_accuracy: 0.8280\n",
            "Epoch 33/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0855 - accuracy: 0.9817 - val_loss: 143.5912 - val_accuracy: 0.8302\n",
            "Epoch 34/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0799 - accuracy: 0.9832 - val_loss: 140.5736 - val_accuracy: 0.8182\n",
            "Epoch 35/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0747 - accuracy: 0.9850 - val_loss: 139.3814 - val_accuracy: 0.8160\n",
            "Epoch 36/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0704 - accuracy: 0.9880 - val_loss: 148.0047 - val_accuracy: 0.8248\n",
            "Epoch 37/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0661 - accuracy: 0.9880 - val_loss: 151.4621 - val_accuracy: 0.8237\n",
            "Epoch 38/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0609 - accuracy: 0.9905 - val_loss: 160.6819 - val_accuracy: 0.8204\n",
            "Epoch 39/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0551 - accuracy: 0.9920 - val_loss: 162.9181 - val_accuracy: 0.8226\n",
            "Epoch 40/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0514 - accuracy: 0.9934 - val_loss: 165.6823 - val_accuracy: 0.8204\n",
            "Epoch 41/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0480 - accuracy: 0.9931 - val_loss: 170.2048 - val_accuracy: 0.8171\n",
            "Epoch 42/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0447 - accuracy: 0.9942 - val_loss: 170.3194 - val_accuracy: 0.8204\n",
            "Epoch 43/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0412 - accuracy: 0.9956 - val_loss: 176.1712 - val_accuracy: 0.8226\n",
            "Epoch 44/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0379 - accuracy: 0.9963 - val_loss: 191.3177 - val_accuracy: 0.8182\n",
            "Epoch 45/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0354 - accuracy: 0.9960 - val_loss: 197.1071 - val_accuracy: 0.8182\n",
            "Epoch 46/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0326 - accuracy: 0.9963 - val_loss: 196.5553 - val_accuracy: 0.8226\n",
            "Epoch 47/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0300 - accuracy: 0.9978 - val_loss: 198.1265 - val_accuracy: 0.8226\n",
            "Epoch 48/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0282 - accuracy: 0.9967 - val_loss: 200.5331 - val_accuracy: 0.8280\n",
            "Epoch 49/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0263 - accuracy: 0.9971 - val_loss: 206.9556 - val_accuracy: 0.8204\n",
            "Epoch 50/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0245 - accuracy: 0.9989 - val_loss: 201.7715 - val_accuracy: 0.8248\n",
            "Epoch 51/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0230 - accuracy: 0.9982 - val_loss: 212.3567 - val_accuracy: 0.8248\n",
            "Epoch 52/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0215 - accuracy: 0.9993 - val_loss: 209.3956 - val_accuracy: 0.8237\n",
            "Epoch 53/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0200 - accuracy: 0.9989 - val_loss: 220.6374 - val_accuracy: 0.8258\n",
            "Epoch 54/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0184 - accuracy: 0.9989 - val_loss: 222.6361 - val_accuracy: 0.8237\n",
            "Epoch 55/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0170 - accuracy: 0.9989 - val_loss: 229.7433 - val_accuracy: 0.8237\n",
            "Epoch 56/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0164 - accuracy: 0.9985 - val_loss: 235.1154 - val_accuracy: 0.8248\n",
            "Epoch 57/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0155 - accuracy: 0.9989 - val_loss: 240.2292 - val_accuracy: 0.8269\n",
            "Epoch 58/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0144 - accuracy: 0.9993 - val_loss: 246.8056 - val_accuracy: 0.8291\n",
            "Epoch 59/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0132 - accuracy: 0.9993 - val_loss: 251.2360 - val_accuracy: 0.8258\n",
            "Epoch 60/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0126 - accuracy: 0.9993 - val_loss: 249.6138 - val_accuracy: 0.8258\n",
            "Epoch 61/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0118 - accuracy: 0.9996 - val_loss: 251.0031 - val_accuracy: 0.8204\n",
            "Epoch 62/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0109 - accuracy: 0.9993 - val_loss: 251.8502 - val_accuracy: 0.8226\n",
            "Epoch 63/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0104 - accuracy: 0.9993 - val_loss: 261.0125 - val_accuracy: 0.8280\n",
            "Epoch 64/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0100 - accuracy: 0.9993 - val_loss: 265.5847 - val_accuracy: 0.8226\n",
            "Epoch 65/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0095 - accuracy: 0.9993 - val_loss: 271.2887 - val_accuracy: 0.8280\n",
            "Epoch 66/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0090 - accuracy: 0.9993 - val_loss: 267.3029 - val_accuracy: 0.8258\n",
            "Epoch 67/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0099 - accuracy: 0.9989 - val_loss: 270.0042 - val_accuracy: 0.8215\n",
            "Epoch 68/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0102 - accuracy: 0.9993 - val_loss: 275.1918 - val_accuracy: 0.8280\n",
            "Epoch 69/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 275.0973 - val_accuracy: 0.8269\n",
            "Epoch 70/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 274.6003 - val_accuracy: 0.8193\n",
            "Epoch 71/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 286.5209 - val_accuracy: 0.8248\n",
            "Epoch 72/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0073 - accuracy: 0.9996 - val_loss: 290.7000 - val_accuracy: 0.8269\n",
            "Epoch 73/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 0.9993 - val_loss: 295.3635 - val_accuracy: 0.8280\n",
            "Epoch 74/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0076 - accuracy: 0.9993 - val_loss: 300.9634 - val_accuracy: 0.8269\n",
            "Epoch 75/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0068 - accuracy: 0.9996 - val_loss: 303.9115 - val_accuracy: 0.8269\n",
            "Epoch 76/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 298.8311 - val_accuracy: 0.8269\n",
            "Epoch 77/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9996 - val_loss: 296.9131 - val_accuracy: 0.8302\n",
            "Epoch 78/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0053 - accuracy: 0.9996 - val_loss: 304.6823 - val_accuracy: 0.8291\n",
            "Epoch 79/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0052 - accuracy: 0.9989 - val_loss: 312.0699 - val_accuracy: 0.8237\n",
            "Epoch 80/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0053 - accuracy: 0.9989 - val_loss: 316.4040 - val_accuracy: 0.8248\n",
            "Epoch 81/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0049 - accuracy: 0.9996 - val_loss: 310.7303 - val_accuracy: 0.8313\n",
            "Epoch 82/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0043 - accuracy: 0.9996 - val_loss: 311.4841 - val_accuracy: 0.8258\n",
            "Epoch 83/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0046 - accuracy: 0.9996 - val_loss: 316.3521 - val_accuracy: 0.8291\n",
            "Epoch 84/100\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0047 - accuracy: 0.9993 - val_loss: 314.1299 - val_accuracy: 0.8313\n",
            "Epoch 85/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0043 - accuracy: 0.9993 - val_loss: 321.9649 - val_accuracy: 0.8258\n",
            "Epoch 86/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 321.9104 - val_accuracy: 0.8280\n",
            "Epoch 87/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0040 - accuracy: 0.9993 - val_loss: 324.7031 - val_accuracy: 0.8313\n",
            "Epoch 88/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0039 - accuracy: 0.9996 - val_loss: 332.1699 - val_accuracy: 0.8302\n",
            "Epoch 89/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0043 - accuracy: 0.9993 - val_loss: 337.7200 - val_accuracy: 0.8248\n",
            "Epoch 90/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 332.7671 - val_accuracy: 0.8313\n",
            "Epoch 91/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 337.6273 - val_accuracy: 0.8313\n",
            "Epoch 92/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0042 - accuracy: 0.9996 - val_loss: 332.4315 - val_accuracy: 0.8302\n",
            "Epoch 93/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0040 - accuracy: 0.9996 - val_loss: 329.8950 - val_accuracy: 0.8291\n",
            "Epoch 94/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0054 - accuracy: 0.9989 - val_loss: 350.8757 - val_accuracy: 0.8237\n",
            "Epoch 95/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0050 - accuracy: 0.9993 - val_loss: 360.2909 - val_accuracy: 0.8258\n",
            "Epoch 96/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0043 - accuracy: 0.9993 - val_loss: 346.0461 - val_accuracy: 0.8269\n",
            "Epoch 97/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 346.4480 - val_accuracy: 0.8280\n",
            "Epoch 98/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 351.8661 - val_accuracy: 0.8280\n",
            "Epoch 99/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 348.5015 - val_accuracy: 0.8302\n",
            "Epoch 100/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 356.0495 - val_accuracy: 0.8258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha8webVjzMBM"
      },
      "source": [
        "The accuracy and val accuracy scores are similar to other classification models. I will keep tinkering with the model to see if I can improve on this any more. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x48uhrBsxGUK",
        "outputId": "f5774c68-b644-43fe-ed39-e1ce8a1e6cb6"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "#layer1\n",
        "model.add(Dense(n_input, input_shape= (n_input,) , activation='relu'))\n",
        "\n",
        "#layer 2\n",
        "model.add(Dense(n_input, activation='relu'))\n",
        "\n",
        "#layer 3\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#compile \n",
        "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "#early stop, monitor val loss\n",
        "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "##fit model, attach the callback\n",
        "h = model.fit(X_train_sc, y_train, validation_data = (X_test_sc, y_test), epochs=100, \n",
        "                          batch_size=None, callbacks = [early_stop])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "86/86 [==============================] - 1s 4ms/step - loss: 0.4374 - accuracy: 0.7970 - val_loss: 35.2143 - val_accuracy: 0.8346\n",
            "Epoch 2/100\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.3275 - accuracy: 0.8518 - val_loss: 50.3498 - val_accuracy: 0.8324\n",
            "Epoch 3/100\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.2731 - accuracy: 0.8777 - val_loss: 71.8628 - val_accuracy: 0.8412\n",
            "Epoch 4/100\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.2321 - accuracy: 0.9047 - val_loss: 67.7274 - val_accuracy: 0.8346\n",
            "Epoch 5/100\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.1971 - accuracy: 0.9193 - val_loss: 58.7539 - val_accuracy: 0.8390\n",
            "Epoch 6/100\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.1646 - accuracy: 0.9379 - val_loss: 70.2463 - val_accuracy: 0.8423\n",
            "Epoch 00006: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bqkETsi0zmy"
      },
      "source": [
        "The accuracy scores slightly improved when early stopping was imlemented but still performing similarly to other classification models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFmrtiXP0zQp"
      },
      "source": [
        "preds = np.round(model.predict(X_test_sc)).astype(int)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TLekFhYz9pi",
        "outputId": "347adbcc-6fc7-4714-eedb-6098609fdfc5"
      },
      "source": [
        "preds[:5]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoyAlohyz9m-",
        "outputId": "4b0859f8-124b-4981-904b-0a7a4916c3ea"
      },
      "source": [
        "print(classification_report(y_test, preds))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.90      0.90       709\n",
            "           1       0.64      0.64      0.64       204\n",
            "\n",
            "    accuracy                           0.84       913\n",
            "   macro avg       0.77      0.77      0.77       913\n",
            "weighted avg       0.84      0.84      0.84       913\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy5N5oA41OnL"
      },
      "source": [
        "We can see from the classification report that this model is performing similar to other classification models with accuracy, however precision is lower with 64% on the target class which is not ideal. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJxJJ3671Yhq"
      },
      "source": [
        "Below, I will try tweaking the model with more dense layers, dropouts, and early stopping. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhmD9_9k1hqS",
        "outputId": "36b87bbd-5027-4cda-dc89-1eecd332ebf7"
      },
      "source": [
        "model_better = Sequential()\n",
        "\n",
        "#layer1\n",
        "model_better.add(Dense(n_input, input_shape= (n_input,) , activation='relu'))\n",
        "\n",
        "#layer 2\n",
        "model_better.add(Dense(80, activation='relu'))\n",
        "model_better.add(Dropout(0.3))\n",
        "\n",
        "#layer 3\n",
        "model_better.add(Dense(n_input, activation='relu'))\n",
        "model_better.add(Dropout(0.3))\n",
        "\n",
        "#layer 4\n",
        "model_better.add(Dense(40, activation='relu'))\n",
        "model_better.add(Dropout(0.3))\n",
        "\n",
        "#layer 5\n",
        "model_better.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#compile \n",
        "model_better.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "#early stop, monitor val loss\n",
        "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "##fit model, attach the callback\n",
        "h = model_better.fit(X_train_sc, y_train, validation_data = (X_test_sc, y_test), epochs=100, \n",
        "                          batch_size=None, callbacks = [early_stop])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "86/86 [==============================] - 1s 3ms/step - loss: 0.5032 - accuracy: 0.7689 - val_loss: 29.6753 - val_accuracy: 0.7864\n",
            "Epoch 2/100\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 0.3990 - accuracy: 0.8039 - val_loss: 49.6552 - val_accuracy: 0.8116\n",
            "Epoch 3/100\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.3407 - accuracy: 0.8408 - val_loss: 83.3935 - val_accuracy: 0.8237\n",
            "Epoch 4/100\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.2959 - accuracy: 0.8664 - val_loss: 74.0227 - val_accuracy: 0.8269\n",
            "Epoch 5/100\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.2613 - accuracy: 0.8832 - val_loss: 94.6854 - val_accuracy: 0.8291\n",
            "Epoch 6/100\n",
            "86/86 [==============================] - 0s 3ms/step - loss: 0.2234 - accuracy: 0.9022 - val_loss: 102.4928 - val_accuracy: 0.8302\n",
            "Epoch 00006: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlPnj2JO2Cb_"
      },
      "source": [
        "We can see from the scores above that we did not improve on the accuracy any more compared to before. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJDzjV2P6YlC"
      },
      "source": [
        "preds = np.round(model_better.predict(X_test_sc)).astype(int)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW6auevE6YSV",
        "outputId": "6993bedf-b2f9-48e3-8907-edf33d3502a1"
      },
      "source": [
        "print(classification_report(y_test, preds))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.89      0.89       709\n",
            "           1       0.62      0.62      0.62       204\n",
            "\n",
            "    accuracy                           0.83       913\n",
            "   macro avg       0.76      0.76      0.76       913\n",
            "weighted avg       0.83      0.83      0.83       913\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDSEKbQL5lKc"
      },
      "source": [
        "Below are plots of the accuracy scores for training and testing to visualize what is happening over epochs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "XKHrjPo31hmi",
        "outputId": "68c3cd08-a807-4767-93d7-e06fe760a61b"
      },
      "source": [
        "plt.plot(h.history['accuracy'], label = 'Train acc')\n",
        "plt.plot(h.history['val_accuracy'], label = 'Val acc')\n",
        "plt.legend();"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjKKYmcz6Aaa"
      },
      "source": [
        "Overall, the neural network classification models were not able to perform as well as the extra trees classifier with accuracy and precision scores. It was good to test the neural network classification model as an option since it is so powerful but ultimately will not use this as the productio model. "
      ]
    }
  ]
}